---
title: "LING698D: Hierarchical modeling"
subtitle: "Day 3"
author: "Scott Jackson"
date: "2026-01-15"
---

# Agenda

1. Questions, review, problems
2. Two-sample t-tests, t vs normal
3. Linear model
4. Exploring Practice data

# Questions

- No questions, all clear

# Loading libraries

- `HistData` is an interesting package that contains a bunch of "classic" historical data sets.
- One of the rare packages that is pretty much just a collection of data sets without any new functions.
- If you didn't realize that R packages often come with pre-loaded data sets, here's the skinny:
  - You can run the function `data()` to list out all of the pre-loaded data sets that are currently available. This list is organized by package.
  - All of these data sets are currently available, meaning that you can use their "namespace" normally (for example, the `sleep` dataframe is virtually always available), but these are not in the "Global Environment", meaning that they don't show up as variable when you use the `ls()` or `objects()` functions.
  - This is why you might not have realized they were available, but it's nice because otherwise they would clutter up your variables workspace.
- From `HistData` we'll use the Galton height data, since it's the classic origin of the work around "regression to the mean."

```{r}
library(tidyverse)
library(HistData)
```

# Simulating two-sample t-tests for power and false positives

## Define some helper functions

- Recall that the core focus should be on **estimates**, and uncertainty around that estimate, which can be quantified as a **standard error**.
- When we divide an estimate by its standard error, we get something that's either assumed to follow a *t* distribution (like in linear model coefficients) or sometimes a *normal* distribution (for logistic model coefficients that are expressed as *z*-scores).
- So often the mathematical trick is to know how to get the standard error.
- The following functions implement the standard math for getting:
  - the standard error of a mean for a single sample (not for a proportion)
  - the standard error of the difference in the means between two independent samples, which in turn requires a formula for calculating...
  - ... a pooled standard deviation when you are comparing two samples.
- Most of the time our model-fitting functions like `lm()`, `glm()`, `lmer()`, `glmer()`, etc. do the math for us.
- So these functions here are just so we can see the math for these simple cases.

```{r}
# standard error of the mean for a single sample
std_err_mean <- function(x) {
    return(sd(x)/sqrt(length(x)))
}

# computing the standard error of a difference requires different formulae
sd_pooled <- function(x, y) {
    return(sqrt(((length(x)-1)*var(x) + (length(y)-1)*var(y))/(length(x) + length(y) - 2)))
}

std_err_diff <- function(x, y) {
    return(sd_pooled(x, y)*sqrt(1/length(x) + 1/length(y)))
}

```

## Run simulation loop for two-sample

The following code chunk implements our now-familiar routine for running simulations:
  1. Set up some parameters
    - here, the mean of an `x` variable,
    - the difference between that mean and the mean of a `y` variable, 
    - the number of observations (here, we are simplifying things by making both samples the same length, but that's not necessary),
    - and the standard deviations (referred to as `sigma`) of both variables 
  2. Set up a data frame with "empty" cells that we intend to "fill in" with simulations
    - here, we'll keep track of the observed difference (which we can compare with our "known" difference that we are setting up as a parameter), 
    - the t-value, 
    - the corresponding p-value, 
    - and just as a demonstration, the t-value we get from R's built-in `t.test()` function.
    - This last one is just to illustrate/confirm that R's function does get us the same results as our "manual" calculations.

The goal of this is just to recap the process using a (very) slightly more complex example, the two-sample t-test.

- Finally, we can use the plots to show the *t*-values (under a null hypothesis) really do follow a *t* distribution more closely than a *normal* distribution, but that the more observations we have, the closer and closer those distributions are to each other.
  - **NB**: to show that the BLUE *t*-distribution line is a better match, you need to simulate using a value of zero for the `difference` variable. This is because the *t* distribution is always centered on zero. If the "true" difference is not zero, then the *t* values will be in the tails of the *t*-distribution, which is why they end up with small *p*-values.
  - Note the use of `dt()` and `dnorm()` to calculate the *theoretical* probability densities of those distributions, which is what gets you the "perfect" distribution curves, rather than the always-slightly-imperfect densities of random vectors generated from those distributions.

```{r}
n_obs <- 100
mu_x <- 30
sigma_x <- 5

difference <- 2

mu_y <- mu_x + difference
sigma_y <- 5

n_sims <- 1e4
results <- data.frame(sim = 1:n_sims,
                      diff = NA,
                      t = NA,
                      t_R = NA,
                      p = NA)

for(this_sim in 1:n_sims) {
    if(this_sim %% 1000 == 0) { cat("on sim#: ", this_sim, "\n") }
    x_vals <- rnorm(n_obs, mean = mu_x, sd = sigma_x)
    y_vals <- rnorm(n_obs, mean = mu_y, sd = sigma_y)
    this_estimate <- mean(y_vals) - mean(x_vals)
    this_se <- std_err_diff(x_vals, y_vals)
    this_t <- this_estimate/this_se
    this_p <- 2*(1-pt(abs(this_t), df = n_obs*2 - 2))
    this_test <- t.test(y_vals, x_vals, var.equal = TRUE)
    results[this_sim, 2:5] <- c(this_estimate, this_t, this_test$statistic,this_p)
}

head(results)
mean(results$p < .05)
mean(results$diff[results$p < .05]) # inflated estimates under low power

ggplot(results, aes(t)) + geom_density()

t_densities <- dt(results$t, df = n_obs*2 - 2)
norm_densities <- dnorm(results$t, mean(results$t), sd(results$t))

ggplot(results, aes(t)) + geom_density() +
    geom_line(aes(y = t_densities), color = "blue", linetype = 2) +
    geom_line(aes(y = norm_densities), color = "red", linetype = 2)

```

# Simple linear models

- Using the `Galton` data from the `HistData` package
- Classic set of height data from parents and children
- Originator of the concept of "regression to the mean"
- Measurements in this data are adjusted for sex (as Galton originally did)
- Shape of `Galton` is one column for parent height and one for child height
- We'll also create a "long" data frame, to have all heights in one column and the "person" (parent vs child) in another.
- Since we're reshaping data, we'll also add an "obs" column to create an ID for each observational pair of parent-child.

```{r}
?Galton
head(Galton)
nrow(Galton)

Galton$obs <- 1:nrow(Galton)

Galton_long <- pivot_longer(Galton,
                            cols = parent:child,
                            names_to = "person",
                            values_to = "height")

```

## Core concepts for a linear model

- The linear model equation:
  - simple line: y = intercept + slope * x
  - with uncertainty: y ~ intercept + slope * x + Error
  - in fancy math equation form: $y ~ \alpha + \beta * x + \epsilon$
  - in R model syntax: y ~ 1 + x
- y is the DV/outcome/response
- x is the IV/predictor (data)
- slope/beta is the "effect" of x on y
- intercept is the "constant", or the **predicted value of y when x is zero**
- the Error term in a standard linear regression is assumed to be:
  - normally distributed
  - with a mean of zero
  - so only the standard deviation of that Error is estimated by the model

## Continuous predictor: regression slope estimate

- Parent's height as a predictor of their child's height

```{r}
galton_lm <- lm(child ~ 1 + parent, data = Galton)
summary(galton_lm)

mean(Galton$parent)
Galton$parent_c <- Galton$parent - mean(Galton$parent)

galton_lm2 <- lm(child ~ 1 + parent_c, data = Galton)
summary(galton_lm2)
```

## Examining the residuals and fitted values as a way of assessing a model fit

- If you just take the model "predictions" without the Error component of the model, those are basically the values that fall on the line described by the linear equation (intercept + slope * predictor).
- In other words, these fitted values fall on the **regression line**.
- The differences between these predicted/fitted values and the actual *observed* values are called the **residuals**.
- **The linear model assumes that residuals (aka errors) are normally distributed, with a mean of zero.**
  - The logic of this assumption follows from the Central Limit Theorem (because errors are the result of many unrelated factors), and errors center around zero because if they weren't, our estimates would have systematic bias. If we have a process that estimates unbiased predictors, errors will average to zero (or numerically veerrrrry close to zero).

- This means it's reassuring if we can plot our residuals and see a normally-distributed pattern.
- Additionally, plotting residuals as a function of fitted values can be another way to diagnose problems.
  - For example, if predictions in some ranges show a different pattern of errors than others, it might give us hints as to where our model is particularly bad, which might give us ideas for how to improve it.
  - In other words, when we put fitted/predicted values on the x-axis and residuals on the y-axis, ideally we see a "flat cloud" that centers vertically around zero, but whatever pattern we see should make sense given our data.

```{r}
Galton$predicted_child <- galton_lm$fitted.values

Galton$resid <- galton_lm$residuals

ggplot(Galton, aes(resid)) + geom_density()

mean(Galton$resid)

resid_densities <- dnorm(Galton$resid, mean = mean(Galton$resid), 
                         sd = sd(Galton$resid))

# superimposing a "true" normal distribution to compare to the residual distribution
ggplot(Galton, aes(resid)) + geom_density() +
    geom_line(aes(y = resid_densities), color = "blue", linetype = 2)

ggplot(Galton, aes(predicted_child, resid)) + geom_point()

ggplot(Galton, aes(parent, child)) + geom_point() +
    geom_smooth(method = "lm")

# "jittering" points (adding a little random "noise" in their position)
# helps to display the data when points are being "overplotted" in overlapping positions
ggplot(Galton, aes(parent, child)) + geom_jitter() +
    geom_smooth(method = "lm")

```


## Categorical predictor: analogous to t-test

- Using `lm()` with a categorical predictor gets the same result as a two-sample t-test

```{r}
head(Galton_long)

galton_diff_lm <- lm(height ~ 1 + person, Galton_long)

summary(galton_diff_lm)
# compare to:
t.test(Galton$parent, Galton$child)

# t- and p-values are identical (not a coincidence!)

```

