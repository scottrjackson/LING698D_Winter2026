---
title: "LING698D: Hierarchical modeling"
subtitle: "Day 2"
author: "Scott Jackson"
date: "2026-01-15"
---

# Agenda

1. Questions, review, problems
2. Two-sample t-tests, t vs normal


# Loading libraries

```{r}
library(tidyverse)
library(HistData)

```

# Simulating two-sample t-tests for power and false positives

## Define some helper functions

```{r}
# standard error of the mean for a single sample
std_err_mean <- function(x) {
    return(sd(x)/sqrt(length(x)))
}

# computing the standard error of a difference requires different formulae
sd_pooled <- function(x, y) {
    return(sqrt(((length(x)-1)*var(x) + (length(y)-1)*var(y))/(length(x) + length(y) - 2)))
}

std_err_diff <- function(x, y) {
    return(sd_pooled(x, y)*sqrt(1/length(x) + 1/length(y)))
}

```

## Run simulation loop for two-sample

Demonstrate:
- two-sample formulas
- equivalence to R's built-in "Student's" t-test
- "inflation" of estimate when power is low
- distribution does follow a *t* distribution better than a normal distribution
  - but they get very similar pretty quickly

```{r}
n_obs <- 5
mu_x <- 30
sigma_x <- 5

difference <- 0

mu_y <- mu_x + difference
sigma_y <- 5

n_sims <- 1e4
results <- data.frame(sim = 1:n_sims,
                      diff = NA,
                      t = NA,
                      #t_R = NA,
                      p = NA)

for(this_sim in 1:n_sims) {
    if(this_sim %% 1000 == 0) { cat("on sim#: ", this_sim, "\n") }
    x_vals <- rnorm(n_obs, mean = mu_x, sd = sigma_x)
    y_vals <- rnorm(n_obs, mean = mu_y, sd = sigma_y)
    this_estimate <- mean(y_vals) - mean(x_vals)
    this_se <- std_err_diff(x_vals, y_vals)
    this_t <- this_estimate/this_se
    this_p <- 2*(1-pt(abs(this_t), df = n_obs*2 - 2))
    this_test <- t.test(y_vals, x_vals, var.equal = TRUE)
    results[this_sim, 2:5] <- c(this_estimate, this_t, this_p, this_test$statistic)
}

head(results)
mean(results$p < .05)
mean(results$diff[results$p < .05]) # inflated estimates under low power

ggplot(results, aes(t)) + geom_density()

t_densities <- dt(results$t, df = n_obs*2 - 2)
norm_densities <- dnorm(results$t, mean(results$t), sd(results$t))

ggplot(results, aes(t)) + geom_density() +
    geom_line(aes(y = t_densities), color = "blue", linetype = 2) +
    geom_line(aes(y = norm_densities), color = "red", linetype = 2)

```

# Simple linear models

- Using the `Galton` data from the `HistData` package
- Classic set of height data from parents and children
- Originator of the concept of "regression to the mean"
- Measurements in this data are adjusted for sex (as Galton originally did)
- Shape of `Galton` is one column for parent height and one for child height
- We'll also create a "long" data frame, to have all heights in one column and the "person" (parent vs child) in another.
- Since we're reshaping data, we'll also add an "obs" column to create an ID for each observational pair of parent-child.

```{r}
?Galton
head(Galton)
nrow(Galton)

Galton$obs <- 1:nrow(Galton)

Galton_long <- pivot_longer(Galton,
                            cols = parent:child,
                            names_to = "person",
                            values_to = "height")

```

## Core concepts for a linear model

- The linear model equation:
  - simple line: y = intercept + slope * x
  - with uncertainty: y ~ intercept + slope * x + Error
  - in fancy math equation form: $y ~ \alpha + \beta * x + \epsilon$
  - in R model syntax: y ~ 1 + x
- y is the DV/outcome/response
- x is the IV/predictor (data)
- slope/beta is the "effect" of x on y
- intercept is the "constant", or the **predicted value of y when x is zero**
- the Error term in a standard linear regression is assumed to be:
  - normally distributed
  - with a mean of zero
  - so only the standard deviation of that Error is estimated by the model

## Continuous predictor: regression slope estimate

- Parent's height as a predictor of their child's height

```{r}


```


## Categorical predictor: analogous to t-test

- Using `lm()` with a categorical predictor gets the same result as a two-sample t-test

```{r}

```


# Practicing with real data: McGowan et al. (2026)

- Copy of Cognition paper loaded on ELMS
- All data and sample code from here: https://osf.io/xej9k/files/osfstorage
- Disclaimer: I am not attempting to criticize or endorse the paper; we are just using this as fodder for practice and illustration exercises.


## Loading the data and renaming columns
```{r, eval = FALSE}
mcgowan <- read_csv("practice_data/Combined exp 2 - dataset - 248 participants.csv")
colnames(mcgowan)
mcgowan <- rename(mcgowan,
                  subject = Participant_ID,
                  sentence = Sentence_Type,
                  exp = Experiment,
                  loc = Location,
                  trial = Trial_Number,
                  RT = Reaction_Time,
                  ACC = Accuracy,
                  rate = Presentation_Rate)
colnames(mcgowan)
head(mcgowan)
```

## Some exploratory data analysis

- histograms!
- consider transformations
- create subsets

```{r}


```
