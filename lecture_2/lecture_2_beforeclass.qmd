---
title: "LING698D: Hierarchical modeling"
subtitle: "Day 2"
author: "Scott Jackson"
date: "2026-01-13"
---

# Agenda

1. Questions, review, problems
2. GitHub
3. Simulating a difference of means: t distribution
4. Simple linear model: Galton height data
5. Data for practice: McGowan et al. 2026

# Loading libraries

```{r}
library(tidyverse)
library(HistData)
library(pwr) # library with some exact-calculation power functions
```

# Review, questions


# GitHub

- Repo URL is here: https://github.com/scottrjackson/LING698D_Winter2026
  - I will update (at least) daily
  - I will also upload files to ELMS
  - I will post links to videos on ELMS describing how to set up git and basic usage, including usage with Positron
  - If there is sufficient interest, I can make a similar video for RStudio


# Simulating a difference of means

## Generating and examining values from a normal distribution

```{r}
n_obs <- 1e5


y_vals <- rnorm(n_obs, mean = 10, sd = 3)
mean(y_vals)
sd(y_vals)
ggplot(data = NULL, aes(y_vals)) + geom_histogram(binwidth = .1)

ggplot(data = NULL, aes(y_vals)) + geom_density()

ggplot(data = NULL, aes(y_vals)) + 
    geom_histogram(aes(y = after_stat(density)), binwidth = .05)

y_densities <- dnorm(y_vals, mean = 10, sd = 3)

(density_histplot <- ggplot(data = NULL, aes(y_vals)) + 
    geom_histogram(aes(y = after_stat(density)), binwidth = .05, fill = "gray") +
    geom_line(aes(y = y_densities), color = "red", linetype = 2) +
    scale_x_continuous(breaks = seq(-2, 22, 3)))

print(density_histplot)

png("example_density_histplot.png", width = 10, height = 5,
    units = "in", res = 300) 
print(density_histplot) 
dev.off()

```




## Uncertainty and hypothesis testing

```{r}
# define standard error of a mean
std_err_mean <- function(x) {
    return(sd(x)/sqrt(length(x)))
}

n_obs <- 100
mu <- 2
sigma <- 5
y_vals <- rnorm(n_obs, mean = mu, sd = sigma)
mean(y_vals)
sd(y_vals)

std_err_mean(y_vals)

(t_val <- mean(y_vals)/std_err_mean(y_vals))
# rt(100, df = n_obs - 1)
2*(1 - pt(t_val, df = n_obs - 1))
t.test(y_vals)


```


## Simulating one-sample t-tests for power and false positives

```{r}

n_obs <- 10
mu <- 2
sigma <- 5
n_sims <- 1e4
results <- data.frame(sim = 1:n_sims,
                      mean = NA,
                      t = NA,
                      p = NA)
for(this_sim in 1:n_sims) {
    y_vals <- rnorm(n_obs, mean = mu, sd = sigma)
    mean_y <- mean(y_vals)
    t_val <- mean(y_vals)/std_err_mean(y_vals)
    p_val <- 2*(1 - pt(abs(t_val), df = n_obs - 1))
    results[this_sim, c("mean", "t", "p")] <- c(mean_y, t_val, p_val)
}

# 2*(1-pt(2, 99))
# 2*(1-pt(abs(-2), 99))
# mean(y_vals)
# sd(y_vals)

head(results)
mean(results$p < .05)

mean(results$mean[results$p < .05])

print("bookmark, delete later")
```


## Simulating two-sample t-tests for power and false positives

```{r}
# computing the standard error of a difference requires different formulae
sd_pooled <- function(x, y) {
    return(sqrt(((length(x)-1)*var(x) + (length(y)-1)*var(y))/(length(x) + length(y) - 2)))
}

std_err_diff <- function(x, y) {
    return(sd_pooled(x, y)*sqrt(1/length(x) + 1/length(y)))
}

```


# Simple linear models

- Using the `Galton` data from the `HistData` package
- Classic set of height data from parents and children
- Originator of the concept of "regression to the mean"
- Measurements in this data are adjusted for sex (as Galton originally did)
- Shape of `Galton` is one column for parent height and one for child height
- We'll also create a "long" data frame, to have all heights in one column and the "person" (parent vs child) in another.
- Since we're reshaping data, we'll also add an "obs" column to create an ID for each observational pair of parent-child.

```{r}
?Galton
head(Galton)
nrow(Galton)

Galton$obs <- 1:nrow(Galton)

Galton_long <- pivot_longer(Galton,
                            cols = parent:child,
                            names_to = "person",
                            values_to = "height")

```

## Core concepts for a linear model

- The linear model equation:
  - simple line: y = intercept + slope * x
  - with uncertainty: y ~ intercept + slope * x + Error
  - in fancy math equation form: $y ~ \alpha + \beta * x + \epsilon$
  - in R model syntax: y ~ 1 + x
- y is the DV/outcome/response
- x is the IV/predictor (data)
- slope/beta is the "effect" of x on y
- intercept is the "constant", or the **predicted value of y when x is zero**
- the Error term in a standard linear regression is assumed to be:
  - normally distributed
  - with a mean of zero
  - so only the standard deviation of that Error is estimated by the model

## Continuous predictor: regression slope estimate

- Parent's height as a predictor of their child's height

```{r}


```


## Categorical predictor: analogous to t-test

- Using `lm()` with a categorical predictor gets the same result as a two-sample t-test

```{r}


```


# Practicing with real data: McGowan et al. (2026)

- Copy of Cognition paper loaded on ELMS
- All data and sample code from here: https://osf.io/xej9k/files/osfstorage
- Disclaimer: I am not attempting to criticize or endorse the paper; we are just using this as fodder for practice and illustration exercises.


## Loading the data and renaming columns
```{r, eval = FALSE}
mcgowan <- read_csv("practice_data/Combined exp 2 - dataset - 248 participants.csv")
colnames(mcgowan)
mcgowan <- rename(mcgowan,
                  subject = Participant_ID,
                  sentence = Sentence_Type,
                  exp = Experiment,
                  loc = Location,
                  trial = Trial_Number,
                  RT = Reaction_Time,
                  ACC = Accuracy,
                  rate = Presentation_Rate)
colnames(mcgowan)
head(mcgowan)
```

## Some exploratory data analysis

```{r}


```

